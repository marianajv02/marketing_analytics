# -*- coding: utf-8 -*-
"""clusteringTask_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19xdkVNPWMdu_c25PmUEmXSof_OoABPn8
"""

import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import seaborn as sns

df = pd.read_csv('clean_train_data.csv')

"""## Clustering"""

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score

def eval_nclusters(model,X_data, min_clusters=3,max_clusters=25):
    metric =[]
    for k in range(min_clusters, max_clusters + 1):
        model = model(k)
        labels = model.fit_predict(X_data)
        metric.append(silhouette_score(X_data, labels))
    sns.lineplot(x = range(min_clusters, max_clusters+1), y=metric, marker='x', linewidth=2, color='b')
    plt.axvline(x=10, color='red', linestyle='--', label='Best number of clusters')
    # Set labels and title
    plt.xlabel('Number of Clusters')
    plt.ylabel('Silhouette score')
    plt.title('Silhouette score Across Different Numbers of Clusters')
    plt.show()

"""## Kmeans"""

keep_cols = ['BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES',
       'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE',
       'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY',
       'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY',
       'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'PAYMENTS',
       'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT', 'TENURE','CREDIT_LIMIT']
# Using the credit limit as feature for the Kmeans as it is important to take into account for segmentation.

scaler = StandardScaler()
X_data = scaler.fit_transform(df[keep_cols])

min_clusters = 3
max_clusters = 30
innertia_metric =[]
silohuette_metric =[]
for k in range(min_clusters, max_clusters + 1):
  model = KMeans(n_clusters =k, n_init = 'auto')
  labels = model.fit_predict(X_data)
  silohuette_metric.append(silhouette_score(X_data, labels))
  innertia_metric.append(model.inertia_)

clusters = range(min_clusters, max_clusters+1)
fig, ax = plt.subplots(1,2, figsize=(16,6))
ax.flatten()
sns.lineplot(x = clusters, y=silohuette_metric, marker='x', linewidth=2, color='b', ax=ax[0])
sns.lineplot(x = clusters, y=innertia_metric, marker='x', linewidth=2, color='b', ax=ax[1])
ax[0].axvline(x=clusters[np.argmax(silohuette_metric)], color='red', linestyle='--', label='Best number of clusters')
ax[1].axvline(x=9, color='red', linestyle='--', label='Best number of clusters',)
# Set labels and title
plt.xlabel('Number of Clusters')
ax[0].set(ylabel='Silohuette Score')
ax[1].set(ylabel='Inertia Score')
plt.suptitle('Score Across Different Numbers of Clusters')
plt.show()

"""Setting best n clusters to 9 as according to both criteria is the best."""

best_n = 9
df['kmeans_9'] = KMeans(n_clusters =9, n_init = 'auto').fit_predict(X_data)

sns.countplot(x='kmeans_9', data=df, order=df['kmeans_9'].value_counts().index, palette='viridis')
plt.title('Distribution of Data Points Across Clusters')
plt.xlabel('Cluster')
plt.ylabel('Count')

fig, ax = plt.subplots(3,3, figsize=(16,12))
ax = ax.flatten()
for i in range(best_n):
    credit_limit_counts = df[df['kmeans_9']==i]['CREDIT_LIMIT'].value_counts()
    # sns.barplot(x=credit_limit_counts.index, y=credit_limit_counts.values, palette='viridis', ax=ax[i])
    sns.histplot(df[df['kmeans_9']==i]['CREDIT_LIMIT'], ax=ax[i])
    ax[i].text(0.5, 0.95, f'No of credit_limits: {len(credit_limit_counts)}', transform=ax[i].transAxes,
               verticalalignment='top', horizontalalignment='center', color='red', fontsize=10)
    # print(len(credit_limit_counts))
    ax[i].set(title=f'Cluster {i}')
    # ax[i].set_xticklabels(ax[i].get_xticklabels(), rotation=45, ha='right')
plt.subplots_adjust(bottom=-0.1)
plt.show()

df.groupby('kmeans_9').agg(['mean','std']).round(4).T

"""## Classification of data to Cluster

### Load libraries
"""

from sklearn.linear_model import RidgeCV, LogisticRegression, LogisticRegressionCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, accuracy_score, precision_recall_curve, precision_score, classification_report
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer, make_column_selector as selector
from sklearn.model_selection import cross_val_score, KFold

"""### Useful functions"""

#transformers
numeric_transf = Pipeline(
    steps=[('imputer', SimpleImputer(strategy='median')), ('scaler',StandardScaler())]
)
# Setup the preprocessing steps
preprocess = ColumnTransformer(
    transformers = [
        ('num', numeric_transf, selector(dtype_exclude='object'))
    ]
)

def create_features(df):
    df['purch_per_trx'] = np.where(df['PURCHASES_TRX']>0,df['PURCHASES']/df['PURCHASES_TRX'], 0)
    df['cash_adv_per_trx'] = np.where(df['CASH_ADVANCE_TRX']>0,df['CASH_ADVANCE']/df['CASH_ADVANCE_TRX'], 0)
    df['BALANCE_by_freq'] = df['BALANCE']*df['BALANCE_FREQUENCY']
    df['ONEOFF_PURCHASES_by_freq'] = df['ONEOFF_PURCHASES']*df['ONEOFF_PURCHASES_FREQUENCY']
    df['INSTALLMENTS_PURCHASES_by_freq'] = df['INSTALLMENTS_PURCHASES']*df['PURCHASES_INSTALLMENTS_FREQUENCY']
    df['CASH_ADVANCE_by_freq'] = df['CASH_ADVANCE']*df['CASH_ADVANCE_FREQUENCY']
    df['interest_p_year'] = np.where(df['TENURE']>0,(df['BALANCE'] - ((df['PURCHASES'] + df['CASH_ADVANCE']) - (df['PAYMENTS'] + df['MINIMUM_PAYMENTS']) )) /df['TENURE'],(df['BALANCE'] - (df['PURCHASES'] + df['CASH_ADVANCE']) - (df['PAYMENTS'] + df['MINIMUM_PAYMENTS']) ))
    return df

def plotImportances(m, keep_cols_engi2):
    # Create a DataFrame with feature names, class labels, and coefficient values
    data = {'Feature': np.tile(keep_cols_engi2, best_n),
            'Class': np.repeat(range(best_n), len(keep_cols_engi2)),
            'Coefficient': np.concatenate(m.named_steps['logit'].coef_)}

    df_plot = pd.DataFrame(data)

    bar_width = 0.8

    sns.catplot(data=df_plot, x='Feature', y='Coefficient', hue='Class', kind='bar', height=8, aspect=2, width=bar_width)

    plt.title('Feature Importance - Logistic Regression Classifier for Different Classes')
    plt.xlabel('Feature')
    plt.ylabel('Coefficient Value')
    plt.xticks(rotation=45, ha='right')
    # plt.legend(title='Class', loc='upper right', bbox_to_anchor=(1.1, 1))

    plt.show()

keep_cols_engi2 = ['PURCHASES', 'PAYMENTS',
       'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT', 'TENURE', 'purch_per_trx',
       'cash_adv_per_trx', 'BALANCE_by_freq', 'ONEOFF_PURCHASES_by_freq',
       'INSTALLMENTS_PURCHASES_by_freq', 'CASH_ADVANCE_by_freq','interest_p_year']

# Create variables by feature engineering
df = create_features(df)

"""### Try with same features as in the clusetering"""

keep_cols1 = ['BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES',
       'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE',
       'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY',
       'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY',
       'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'PAYMENTS',
       'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT', 'TENURE']

X = df[keep_cols1]
# The task will be to classify the point to a cluster
y = df['kmeans_9']
X_train, X_test, y_train, y_test = train_test_split(X,y, random_state= 123)

steps = [('preprocess', preprocess),('logit', LogisticRegressionCV(cv=3, max_iter=10000))]
m = Pipeline(steps)
m.fit(X_train, y_train)
y_pred = m.predict(X_test)

print(classification_report(y_test, y_pred))

"""The result is acceptable for all the clusters, so we could classify correctly the new data to the right cluster, above 90% of preccision."""

best_n = 9

plotImportances(m, keep_cols1)

"""From this feature importance we can describe the different clusters.

For example, the cluster 5 (brown) are customers with the lowest number of years Tenure, low payments, and low purchase frequency. However this customers do frequech cash advance of not too high amounts.
"""

sns.boxplot(x='kmeans_9', y ='CREDIT_LIMIT', data=df)

"""Here just to show the clear separation of credit_limit between clusters. We expect cluster 4 to get the best credit limit.

## Assign new customers
"""

new_df = pd.read_csv('new_data.csv')

X = new_df[keep_cols1]
y_pred_new = m.predict(X)

y_pred_new

new_df['cluster'] = y_pred_new

sns.countplot(x='cluster', data=new_df, order=new_df['cluster'].value_counts().index, palette='viridis')
plt.title('Distribution of Data Points Across Clusters')
plt.xlabel('Cluster')
plt.ylabel('Count')

"""## Save it to csv including the cluster"""

new_df.to_csv('new_data_clustered.csv')

"""# Predict credit_limit

## Regression Model per cluster
"""

def model_results(y_test, y_pred2, model=''):
    print('RMSE:', mean_squared_error(y_test, y_pred2, squared=False))
    plt.figure(figsize=(8, 8))
    sns.scatterplot(x=y_test, y=y_pred2, color='blue', alpha=0.7)
    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2)
    plt.xlabel('Real Values (y_test)')
    plt.ylabel('Predicted Values (y_pred)')
    plt.title(f'Real vs. Predicted Values {model}')
    plt.show()

from sklearn.ensemble import GradientBoostingRegressor

params = {'n_estimators': 600,
 'min_samples_split': 16,
 'max_depth': 12,
 'learning_rate': 0.01,
 "loss": "huber",}

target = 'CREDIT_LIMIT'
X = df[keep_cols1]
y = df[target]
X_train, X_test, y_train, y_test = train_test_split(X,y, random_state= 123)

stepsgb = [('preprocess', preprocess),('gb', GradientBoostingRegressor(**params))]
reg = Pipeline(stepsgb)

reg=[]
for i in range(0,9):
    reg.append(Pipeline(stepsgb))
    X = df[df['kmeans_9'] == i][keep_cols1]
    y = df[df['kmeans_9'] == i][target]
    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state= 123)
    reg[i].fit(X_train, y_train)
    y_pred = reg[i].predict(X_test)
    model_results(y_test, y_pred, f'GB Feat Engin Cluster {i}')

"""## Predicting new  Data"""

def predict_credit(data, cluster):
    if isinstance(data, pd.Series):
        data = data.to_frame().T
    model = reg[cluster]
    credit_limit = model.predict(data)
    return credit_limit

df_predict = new_df[keep_cols1]
for i in range(len(new_df)):
    predicted_credit = predict_credit(df_predict.loc[i], new_df.loc[i, 'cluster'])
    new_df.at[i, 'CREDIT_PREDICTED'] = predicted_credit

new_df['CREDIT_PREDICTED']

new_df.to_csv('new_data_with_credit_limit.csv')

fig, ax = plt.subplots(3,3, figsize=(16,12))
ax = ax.flatten()
for i in range(best_n):
    credit_limit_counts = new_df[new_df['cluster']==i]['CREDIT_PREDICTED'].value_counts()
    # sns.barplot(x=credit_limit_counts.index, y=credit_limit_counts.values, palette='viridis', ax=ax[i])
    sns.histplot(new_df[new_df['cluster']==i]['CREDIT_PREDICTED'], ax=ax[i])
    ax[i].text(0.5, 0.95, f'No of credit_limits: {len(credit_limit_counts)}', transform=ax[i].transAxes,
               verticalalignment='top', horizontalalignment='center', color='red', fontsize=10)
    ax[i].set(title=f'Cluster {i}')
plt.subplots_adjust(bottom=-0.1)
plt.show()

"""## Trying a classification model

First select only the credit limits with more than 10 rows within it
"""

credit_limit_counts = df['CREDIT_LIMIT'].value_counts()
# Filter the counts to include only those greater than 10
filtered_credit_limit_counts = credit_limit_counts[credit_limit_counts > 10]

# Split the dataframe into those in the credit_limit with more than 10 counts and those with less
mask_classification = df['CREDIT_LIMIT'].isin(filtered_credit_limit_counts.index)
df_to_class = df[mask_classification]
df_to_reg = df[~mask_classification]

"""Little comparison of both"""

df_to_class.describe()

df_to_reg.describe()

X = df_to_class[keep_cols1]
# The task will be to classify the point to a credit limit
y = df_to_class['CREDIT_LIMIT']
X_train, X_test, y_train, y_test = train_test_split(X,y, random_state= 123)

steps = [('preprocess', preprocess),('logit', LogisticRegressionCV(cv=3, max_iter=10000))]
m = Pipeline(steps)
m.fit(X_train, y_train)
y_pred = m.predict(X_test)

print(classification_report(y_test, y_pred))

"""It was a good learning trying"""